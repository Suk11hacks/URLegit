# -*- coding: utf-8 -*-
"""URLegit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e7pC6BrxrA-aA55bcQlDq5AeGQMId3AY

# The model:
"""

#!pip install -q pandas scikit-learn joblib tldextract


import pandas as pd
import re
import tldextract

uploaded_csv = st.file_uploader("üìÅ Upload CSV with a column named 'url'", type=["csv"])

if uploaded_csv is not None:
    try:
        df = pd.read_csv(uploaded_csv)
        if "url" not in df.columns:
            st.error("‚ùå CSV must contain a column named 'url'.")
        else:
            features = pd.DataFrame([extract_features(u) for u in df['url']])
            df['prediction'] = model.predict(features)
            st.dataframe(df[['url', 'prediction']])

            malicious_urls = df[df['prediction'] == 1]['url']
            for url in malicious_urls:
                insert_url(url)
            st.success(f"‚úÖ Stored {len(malicious_urls)} malicious URLs.")
    except Exception as e:
        st.error(f"‚ùå Failed to process file: {e}")

# ‚úÖ Step 4: Preprocess data
# Keep only relevant columns (assumes 'URL' and 'Label' columns exist)
df = df[['url', 'type']].rename(columns={'url': 'url', 'type': 'label'})
df = df.dropna()
df['label'] = df['label'].apply(lambda x: 1 if str(x).lower() == 'malicious' else 0)

# ‚úÖ Step 5: Feature extraction
def extract_features(url):
    ext = tldextract.extract(url)
    domain = ext.domain
    suffix = ext.suffix
    return {
        "url_length": len(url),
        "hostname_length": len(domain),
        "has_https": int("https" in url),
        "num_dots": url.count('.'),
        "num_hyphens": url.count('-'),
        "num_digits": sum(c.isdigit() for c in url),
        "has_ip": int(bool(re.search(r'\d+\.\d+\.\d+\.\d+', url))),
        "has_at_symbol": int("@" in url),
        "has_exe": int(".exe" in url),
        "suffix_length": len(suffix)
    }

# Apply feature extraction
df['features'] = df['url'].apply(extract_features)
features_df = pd.DataFrame(df['features'].tolist())
X = features_df
y = df['label']

# ‚úÖ Step 6: Train the Random Forest model
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# ‚úÖ Step 7: Evaluate the model
print("‚úÖ Model Accuracy:", clf.score(X_test, y_test))

# ‚úÖ Step 8: Export the model
import joblib
joblib.dump(clf, "rf_url_model.pkl")

# ‚úÖ Step 9: Download the model to your local system
files.download("rf_url_model.pkl")

"""# The app"""

import streamlit as st
import pandas as pd
import joblib
import sqlite3
import re
import tldextract
from io import BytesIO
import requests

# ---------- Constants ----------
MODEL_URL = "https://github.com/yourusername/malicious-url-app/raw/main/model/rf_url_model.pkl"  # Optional remote model URL
DB_PATH = "malicious_urls.db"

# ---------- Feature Extraction ----------
def extract_features(url):
    ext = tldextract.extract(url)
    domain = ext.domain
    suffix = ext.suffix
    return {
        "url_length": len(url),
        "hostname_length": len(domain),
        "has_https": int("https" in url),
        "num_dots": url.count('.'),
        "num_hyphens": url.count('-'),
        "num_digits": sum(c.isdigit() for c in url),
        "has_ip": int(bool(re.search(r'\d+\.\d+\.\d+\.\d+', url))),
        "has_at_symbol": int("@" in url),
        "has_exe": int(".exe" in url),
        "suffix_length": len(suffix)
    }

# ---------- Model Loader ----------
@st.cache_resource
def load_model():
    try:
        response = requests.get(MODEL_URL)
        return joblib.load(BytesIO(response.content))
    except Exception:
        return None

# ---------- SQLite Database ----------
def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS malicious_urls (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE
        )
    """)
    conn.commit()
    conn.close()

def insert_url(url):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("INSERT OR IGNORE INTO malicious_urls (url) VALUES (?)", (url,))
    conn.commit()
    conn.close()

def get_logged_urls():
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql("SELECT url FROM malicious_urls", conn)
    conn.close()
    return df

# ---------- Streamlit App ----------
def main():
    st.set_page_config(page_title="Malicious URL Detector", page_icon="üõ°Ô∏è")
    st.title("üõ°Ô∏è Malicious URL Detection")

    init_db()

    with st.sidebar:
        st.header("üì• Upload Trained Model")
        model_file = st.file_uploader("Upload a .pkl model", type=["pkl"])
        if model_file:
            model = joblib.load(model_file)
            st.success("‚úÖ Model uploaded successfully")
        else:
            model = load_model()
            if model:
                st.info("üì° Using model from remote URL")
            else:
                st.warning("‚ö†Ô∏è No model available. Please upload one.")

    tab1, tab2, tab3 = st.tabs(["üîç Check URL", "üìÅ Upload CSV", "üßæ View Logs"])

    # --------- Tab 1: Manual URL Check ---------
    with tab1:
        url = st.text_input("Enter a URL to check:")
        if url and model:
            features = pd.DataFrame([extract_features(url)])
            prediction = model.predict(features)[0]
            label = "üü• Malicious" if prediction == 1 else "üü© Safe"
            st.subheader(f"Prediction: {label}")
            st.write("Extracted Features", features)

            if prediction == 1:
                insert_url(url)
                st.success("üö® Malicious URL logged in database.")

    # --------- Tab 2: CSV Upload ---------
    with tab2:
        uploaded_csv = st.file_uploader("Upload CSV with a column named 'url'", type=["csv"])
        if uploaded_csv and model:
            try:
                df = pd.read_csv(uploaded_csv)
                if "url" not in df.columns:
                    st.error("‚ùå The CSV must contain a 'url' column.")
                else:
                    features = pd.DataFrame([extract_features(u) for u in df['url']])
                    df['prediction'] = model.predict(features)
                    st.dataframe(df[['url', 'prediction']])

                    malicious_urls = df[df['prediction'] == 1]['url']
                    for u in malicious_urls:
                        insert_url(u)
                    st.success(f"üîí Stored {len(malicious_urls)} malicious URLs.")
            except Exception as e:
                st.error(f"Error processing file: {e}")

    # --------- Tab 3: Logs ---------
    with tab3:
        if st.button("üîç Show Logged Malicious URLs"):
            logs = get_logged_urls()
            st.dataframe(logs)

if __name__ == "__main__":
    main()

